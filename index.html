<!doctype html>
<html>

<head>
  <title>AI Assistant</title>
  <meta charset="utf-8" />
  <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    /* Global box-sizing */
    *,
    *::before,
    *::after {
      box-sizing: border-box;
    }

    /* Body and main container styling */
    body,
    main {
      margin: 0;
      padding: 0;
      min-width: 100%;
      min-height: 100vh; /* Full viewport height */
      font-family: sans-serif;
      text-align: center;
      color: #fff; /* White text */
      background: #000; /* Black background */
    }

    /* Styling for the "start" button */
    button {
      position: absolute;
      left: 50%;
      top: 50%;
      width: 5em;
      height: 2em;
      margin-left: -2.5em; /* Center horizontally */
      margin-top: -1em; /* Center vertically */
      z-index: 100; /* Ensure it's above other elements */
      padding: .25em .5em;
      color: #fff;
      background: #000;
      border: 1px solid #fff;
      border-radius: 4px;
      cursor: pointer;
      font-size: 1.15em;
      font-weight: 200;
      box-shadow: 0 0 10px rgba(255, 255, 255, 0.5); /* Subtle white glow */
      transition: box-shadow .5s; /* Smooth transition for hover effect */
    }

    button:hover {
      box-shadow: 0 0 30px 5px rgba(255, 255, 255, 0.75); /* Enhanced glow on hover */
    }

    /* Styling for the main visualizer area */
    main {
      position: relative;
      display: flex; /* Use flexbox to arrange visual elements */
      justify-content: center; /* Center horizontally */
      align-items: center; /* Center vertically */
    }

    /* Styling for individual bar elements within the visualizer */
    main>div {
      display: inline-block;
      width: 3px; /* Width of each bar */
      height: 100px; /* Base height of each bar */
      margin: 0 7px; /* Spacing between bars */
      background: currentColor; /* Inherits color from parent (main) */
      transform: scaleY(.5); /* Start with half height */
      opacity: .25; /* Start with low opacity */
    }

    /* Error state styling for the main visualizer */
    main.error {
      color: #f7451d; /* Red color for errors */
      min-width: 20em;
      max-width: 30em;
      margin: 0 auto;
      white-space: pre-line; /* Preserve line breaks in error message */
    }

    /* Styling for the transcription display */
    #transcript {
      position: fixed; /* Fixed position relative to viewport */
      top: 60%; /* Positioned lower on the screen */
      width: 100vw;
      padding-left: 20vw;
      padding-right: 20vw;
      font-size: x-large;
    }
  </style>
</head>

<body>
  <main>
    <!-- The button to initialize the audio visualizer -->
    <button onclick="init()">start</button>
  </main>
  <!-- Div to display transcribed text (though not implemented in this version's JS) -->
  <div id="transcript"></div>
</body>

<script>
  /**
   * AudioVisualizer class to capture microphone input and process audio data
   * for visualization.
   */
  class AudioVisualizer {
    /**
     * @param {AudioContext} audioContext - The Web Audio API AudioContext.
     * @param {function} processFrame - Callback function to process frequency data.
     * @param {function} processError - Callback function for microphone access errors.
     */
    constructor(audioContext, processFrame, processError) {
      this.audioContext = audioContext;
      this.processFrame = processFrame;
      this.connectStream = this.connectStream.bind(this); // Bind 'this' for callback context

      // Request microphone access
      navigator.mediaDevices.getUserMedia({ audio: true, video: false })
        .then(this.connectStream) // On success, connect the audio stream
        .catch((error) => {
          if (processError) {
            processError(error); // On error, call the error handler
          }
        });
    }

    /**
     * Connects the media stream (microphone) to the analyser node.
     * @param {MediaStream} stream - The audio media stream from getUserMedia.
     */
    connectStream(stream) {
      this.analyser = this.audioContext.createAnalyser(); // Create an AnalyserNode
      const source = this.audioContext.createMediaStreamSource(stream); // Create audio source from stream
      console.log("Audio source connected:", source); // Log for debugging

      source.connect(this.analyser); // Connect the microphone input to the analyser
      this.analyser.smoothingTimeConstant = 0.5; // Smooths out the visualizer's motion
      this.analyser.fftSize = 32; // Defines the size of the FFT for frequency data (smaller for less bars)

      this.initRenderLoop(); // Start the visualization rendering loop
    }

    /**
     * Initializes the rendering loop to continuously get audio data and update visuals.
     */
    initRenderLoop() {
      // Create a Uint8Array to hold the frequency data
      const frequencyData = new Uint8Array(this.analyser.frequencyBinCount);
      const processFrame = this.processFrame || (() => { }); // Use provided processing function or a dummy

      // Define the frame rendering function
      const renderFrame = () => {
        this.analyser.getByteFrequencyData(frequencyData); // Populate frequencyData array
        processFrame(frequencyData); // Process the data for visualization

        requestAnimationFrame(renderFrame); // Request the next animation frame
      };
      requestAnimationFrame(renderFrame); // Start the animation loop
    }
  }

  // --- DOM Manipulation for Visualizer Bars ---
  const visualMainElement = document.querySelector('main'); // The main container for the bars
  const visualValueCount = 16; // Number of bars to display
  let visualElements; // Stores references to the created div elements

  /**
   * Creates the initial div elements (bars) for the visualizer.
   */
  const createDOMElements = () => {
    visualMainElement.innerHTML = ''; // Clear existing content (like the start button)
    for (let i = 0; i < visualValueCount; ++i) {
      const elm = document.createElement('div');
      visualMainElement.appendChild(elm); // Append div to the main container
    }
    visualElements = document.querySelectorAll('main div'); // Get references to all created divs
  };
  createDOMElements(); // Call once on page load to create initial static bars

  // --- Main Initialization Function ---
  const init = () => {
    // Clear the main element and re-create bars (removes the "start" button)
    visualMainElement.innerHTML = '';
    createDOMElements();

    // Create a new AudioContext
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();

    // Mapping to reorder frequency data for a more visually appealing effect
    const dataMap = {
      0: 15, 1: 10, 2: 8, 3: 9, 4: 6, 5: 5, 6: 2, 7: 1, 8: 0,
      9: 4, 10: 3, 11: 7, 12: 11, 13: 12, 14: 13, 15: 14
    };

    /**
     * Processes audio frequency data to update the visualizer bar heights and opacities.
     * @param {Uint8Array} data - The frequency data array.
     */
    const processFrame = (data) => {
      const values = Object.values(data); // Get numeric values from the array
      for (let i = 0; i < visualValueCount; ++i) {
        // Map the incoming audio frequency data to specific bars for visual effect
        const value = values[dataMap[i]] / 255; // Normalize value (0-1)

        const elmStyles = visualElements[i].style;
        elmStyles.transform = `scaleY( ${value} )`; // Scale bar height
        elmStyles.opacity = Math.max(.25, value); // Adjust opacity (min 0.25)
      }
    };

    /**
     * Handles errors during microphone access.
     * @param {Error} error - The error object.
     */
    const processError = (error) => {
      console.error("Microphone access error:", error);
      visualMainElement.classList.add('error');
      visualMainElement.innerText = 'Please allow access to your microphone in order to see this demo.\nNothing bad is going to happen... hopefully :P\nError: ' + error.name;
    };

    // Instantiate the AudioVisualizer to start microphone input and visualization
    new AudioVisualizer(audioContext, processFrame, processError);

    // --- Backend Transcription (COMMENTED OUT for now as per your request) ---
    // The original code here attempted to fetch from backend ASR endpoints.
    // For this minimalist demo focusing on visuals, we remove these calls.
    // The #transcript div will remain empty unless you add client-side Speech Recognition later.

    /*
    const startTranscriptions = () => {
      fetch("/start_asr").then(res => res.json()).then(data => console.log(data))
      setInterval(() => {
        fetch("/get_audio").then(res => res.json()).then(data => {
          let doc = document.getElementById("transcript")
          if (data !== "") {
            doc.innerHTML = data
          }
        })
      }, 100)
    };
    startTranscriptions();
    */
  };
</script>

</html>
